{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import neurokit2 as nk\n",
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_folder(folder_path, start_path='/content/drive'):\n",
    "  '''\n",
    "    :file_path: The complete path of the folder that includes the files.\n",
    "    :start_path: The root folder on Drive from which we start the search.\n",
    "  '''\n",
    "  #Search the root folder of the data and if found, continue searching the full path.\n",
    "  start_data_folder = folder_path.split('/')[0]\n",
    "  for root, dirs, files in os.walk(start_path):\n",
    "      if start_data_folder in dirs:\n",
    "          return os.path.join(root, folder_path)\n",
    "  raise Exception(f\"{start_data_folder} folder not found. Please check the directory structure.\")\n",
    "\n",
    "def read_store_files(files_path, file_type='.csv'):\n",
    "  '''\n",
    "    :files_path: The complete Drive path that includes the files\n",
    "  '''\n",
    "\n",
    "  data_list = []\n",
    "  files_names = []\n",
    "  for file in os.listdir(files_path):\n",
    "    if file.endswith(file_type):\n",
    "      file_path = os.path.join(files_path, file)\n",
    "      if file_type == '.csv':\n",
    "        data = pd.read_csv(file_path)\n",
    "      elif file_type == '.xlsx':\n",
    "        data = pd.read_excel(file_path)\n",
    "\n",
    "      data_list.append(data)\n",
    "      files_names.append(file)\n",
    "\n",
    "  return data_list, files_names\n",
    "\n",
    "def zero_pad_filenames(annotation_files_names):\n",
    "    # Create a list to hold the zero-padded filenames\n",
    "    padded_filenames = []\n",
    "\n",
    "    for filename in annotation_files_names:\n",
    "        # Extract the number part from the filename\n",
    "        parts = re.split(r'(\\d+)', filename)\n",
    "        # Zero-pad the number part\n",
    "        parts[1] = parts[1].zfill(2)\n",
    "        # Reconstruct the filename\n",
    "        padded_filename = ''.join(parts)\n",
    "        padded_filenames.append(padded_filename)\n",
    "\n",
    "    return padded_filenames\n",
    "\n",
    "def sort_data(files_names, data):\n",
    "    # Get the zero-padded filenames\n",
    "    padded_filenames = zero_pad_filenames(files_names)\n",
    "    \n",
    "    # Create a list of tuples (padded_filename, original_filename, annotation_data)\n",
    "    combined_list = list(zip(padded_filenames, files_names, data))\n",
    "\n",
    "    # Sort the combined list based on the zero-padded filenames\n",
    "    combined_list.sort()\n",
    "\n",
    "    # Unzip the sorted list back into the separate components\n",
    "    sorted_filenames = [original_filename for _, original_filename, _ in combined_list]\n",
    "    sorted_data = [data for _, _, data in combined_list]\n",
    "\n",
    "    return sorted_filenames, sorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path of Annotation files: CASE_full/data/interpolated/annotations\n",
      "Path of Physiological files: CASE_full/data/interpolated/physiological\n",
      "Path of metadata files: CASE_full/metadata\n",
      "Number of Annotation files: 30\n",
      "Names: ['sub_1.csv', 'sub_2.csv', 'sub_3.csv', 'sub_4.csv', 'sub_5.csv', 'sub_6.csv', 'sub_7.csv', 'sub_8.csv', 'sub_9.csv', 'sub_10.csv', 'sub_11.csv', 'sub_12.csv', 'sub_13.csv', 'sub_14.csv', 'sub_15.csv', 'sub_16.csv', 'sub_17.csv', 'sub_18.csv', 'sub_19.csv', 'sub_20.csv', 'sub_21.csv', 'sub_22.csv', 'sub_23.csv', 'sub_24.csv', 'sub_25.csv', 'sub_26.csv', 'sub_27.csv', 'sub_28.csv', 'sub_29.csv', 'sub_30.csv']\n",
      "Number of Physiological files: 30\n",
      "Names: ['sub_1.csv', 'sub_2.csv', 'sub_3.csv', 'sub_4.csv', 'sub_5.csv', 'sub_6.csv', 'sub_7.csv', 'sub_8.csv', 'sub_9.csv', 'sub_10.csv', 'sub_11.csv', 'sub_12.csv', 'sub_13.csv', 'sub_14.csv', 'sub_15.csv', 'sub_16.csv', 'sub_17.csv', 'sub_18.csv', 'sub_19.csv', 'sub_20.csv', 'sub_21.csv', 'sub_22.csv', 'sub_23.csv', 'sub_24.csv', 'sub_25.csv', 'sub_26.csv', 'sub_27.csv', 'sub_28.csv', 'sub_29.csv', 'sub_30.csv']\n",
      "Metadata names: ['participants.xlsx', 'seqs_order_num.xlsx', 'videos.xlsx', 'videos_duration_num.xlsx']\n"
     ]
    }
   ],
   "source": [
    "# Retrieve folders\n",
    "annotations_folder = 'CASE_full/data/interpolated/annotations'\n",
    "physiological_folder = 'CASE_full/data/interpolated/physiological'\n",
    "metadata_folder = 'CASE_full/metadata'\n",
    "\n",
    "print(f'Path of Annotation files: {annotations_folder}')\n",
    "print(f'Path of Physiological files: {physiological_folder}')\n",
    "print(f'Path of metadata files: {metadata_folder}')\n",
    "\n",
    "# Retrieve data files\n",
    "annotation_data, annotation_files_names = read_store_files(annotations_folder)\n",
    "physio_data, physio_files_names = read_store_files(physiological_folder)\n",
    "\n",
    "annotation_files_names, annotation_data = sort_data(annotation_files_names, annotation_data)\n",
    "physio_files_names, physio_data = sort_data(physio_files_names, physio_data)\n",
    "\n",
    "print(f'Number of Annotation files: {len(annotation_data)}\\nNames: {annotation_files_names}')\n",
    "print(f'Number of Physiological files: {len(physio_data)}\\nNames: {physio_files_names}')\n",
    "\n",
    "# Retrieve metadata\n",
    "metadata, metadata_names = read_store_files(metadata_folder,'.xlsx')\n",
    "print(f'Metadata names: {metadata_names}')\n",
    "\n",
    "# Strip column names\n",
    "for i in range(len(metadata)):\n",
    "  metadata[i].columns = metadata[i].columns.str.strip()\n",
    "\n",
    "for i, file in enumerate(metadata_names):\n",
    "  if file == 'videos.xlsx':\n",
    "    videos_data = metadata[i].drop(metadata[i].columns[2:], axis=1).drop([0]).rename(columns={'Video-label': 'label', 'Video-ID': 'video_id'}).dropna()\n",
    "    videos_data['video_id'] = videos_data['video_id'].astype(int)\n",
    "  elif file == 'participants.xlsx':\n",
    "    participant_data = metadata[i].rename(columns={'Participant-ID': 'participant_id', 'Age-Group': 'age_group', 'Video Sequence Used': 'sequence'})\n",
    "\n",
    "  elif file == 'videos_duration_num.xlsx':\n",
    "    duration_data = metadata[i].rename(columns={'video-ID': 'video_id', 'video-duration (in ms)': 'duration'})\n",
    "\n",
    "  elif file == 'seqs_order_num.xlsx':\n",
    "    sequence_order_data = metadata[i]\n",
    "\n",
    "\n",
    "del metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signal_segment(raw_signal, cleaned_signal, title, start_time, duration, sampling_rate=1000):\n",
    "    \"\"\" Utility. Makes two graphs of a raw and a cleaned signal for demonstration \"\"\"\n",
    "    start_sample = int(start_time * sampling_rate)\n",
    "    end_sample = int((start_time + duration) * sampling_rate)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(raw_signal[start_sample:end_sample], label='Raw Signal')\n",
    "    plt.title(f'Raw {title} Segment ({start_time}-{start_time + duration} sec)')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(cleaned_signal[start_sample:end_sample], label='Cleaned Signal')\n",
    "    plt.title(f'Cleaned {title} Segment ({start_time}-{start_time + duration} sec)')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def extract_segments(cleaned_signal, start_time, end_time, original_timestamps):\n",
    "    \"\"\" Utility. Extracts the signal segment given timestamps \"\"\"\n",
    "    start_index = original_timestamps[original_timestamps >= start_time].index[0]\n",
    "    end_index = original_timestamps[original_timestamps <= end_time].index[-1]\n",
    "\n",
    "    segment = cleaned_signal[start_index:end_index+1]\n",
    "    return segment\n",
    "\n",
    "def preprocess_signal(signal, data_type, visualization=False):\n",
    "    \"\"\" Just cleans the signal (NOTE: ok for now).\n",
    "    Set visualization to True to see a before-after example\n",
    "    \"\"\"\n",
    "    if data_type == \"ecg\":\n",
    "        cleaned_signal = nk.ecg_clean(signal, sampling_rate=1000)\n",
    "    elif data_type == \"gsr\":\n",
    "        cleaned_signal = nk.eda_clean(signal, sampling_rate=1000)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported data type\")\n",
    "\n",
    "    if visualization:\n",
    "        # Plot a 5-second segment for demonstration\n",
    "        plot_signal_segment(signal, cleaned_signal, 'Signal', start_time=400, duration=5 if data_type=='ecg' else 200)\n",
    "    return cleaned_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(segment, data_type, sampling_rate=1000):\n",
    "    \"\"\" Extracts ECG and GSR features.\n",
    "\n",
    "    ECG:\n",
    "    - Heart Rate: Mean heart rate over the segment.\n",
    "    - HRV SDNN: Standard deviation of NN intervals (HRV time-domain).\n",
    "    - Mean RR Interval: Mean of RR intervals.\n",
    "    - Poincaré SD1: Standard deviation of the points perpendicular to the line of identity in the Poincaré plot (HRV non-linear).\n",
    "    - LF/HF Ratio: Ratio of low-frequency to high-frequency power (HRV frequency-domain).\n",
    "    - RSA: Respiratory sinus arrhythmia, approximated by RMSSD.\n",
    "    - PSD Total Power: Total power of the power spectral density.\n",
    "    - PSD LF Power: Power in the low-frequency band of the PSD.\n",
    "    - PSD HF Power: Power in the high-frequency band of the PSD.\n",
    "\n",
    "    GSR:\n",
    "    # TODO:\n",
    "    - SCL: Skin conductance level (mean tonic level).\n",
    "    - SCR: Skin conductance response (mean phasic level).\n",
    "    - Peak Amplitude: Maximum amplitude of the SCR peaks.\n",
    "    - PSD Total Power: Total power of the power spectral density.\n",
    "    - PSD LF Power: Power in the low-frequency band of the PSD.\n",
    "    - PSD HF Power: Power in the high-frequency band of the PSD.\n",
    "\n",
    "    Parameters:\n",
    "    - segment: Array-like, the segment of the signal to analyze.\n",
    "    - data_type: str, type of the signal (\"ecg\" or \"gsr\").\n",
    "    - sampling_rate: int, sampling rate of the signal in Hz.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing the extracted features.\n",
    "    \"\"\"\n",
    "    if data_type == \"ecg\":\n",
    "        signals, info = nk.ecg_process(segment, sampling_rate=sampling_rate)\n",
    "        heart_rate = signals['ECG_Rate']\n",
    "\n",
    "        peaks, info = nk.ecg_peaks(segment, sampling_rate=sampling_rate)\n",
    "        hrv_time = nk.hrv_time(peaks, sampling_rate=sampling_rate).iloc[0].to_dict()\n",
    "        hrv_nonlinear = nk.hrv_nonlinear(peaks, sampling_rate=sampling_rate).iloc[0].to_dict()\n",
    "        hrv_frequency = nk.hrv_frequency(peaks, sampling_rate=sampling_rate).iloc[0].to_dict()\n",
    "\n",
    "        psd = nk.signal_psd(segment, sampling_rate=sampling_rate)\n",
    "\n",
    "        features = {\n",
    "            \"Heart Rate\": heart_rate.mean(),\n",
    "            \"HRV SDNN\": hrv_time['HRV_SDNN'],\n",
    "            \"Mean RR Interval\": hrv_time['HRV_MeanNN'],\n",
    "            \"Poincaré SD1\": hrv_nonlinear['HRV_SD1'],\n",
    "            \"LF/HF Ratio\": hrv_frequency['HRV_LFHF'],\n",
    "            \"RSA\": hrv_time['HRV_RMSSD'],\n",
    "            \"PSD Total Power\": psd['Power'].sum(),\n",
    "            \"PSD LF Power\": psd['Power'][psd['Frequency'] <= 0.15].sum(),\n",
    "            \"PSD HF Power\": psd['Power'][(psd['Frequency'] > 0.15) & (psd['Frequency'] <= 0.4)].sum()\n",
    "        }\n",
    "    elif data_type == \"gsr\":\n",
    "        signals, info = nk.eda_process(segment, sampling_rate=sampling_rate)\n",
    "        scl = signals['EDA_Tonic'].mean()\n",
    "        scr = signals['EDA_Phasic'].mean()\n",
    "\n",
    "        peaks, info = nk.eda_peaks(segment, sampling_rate=sampling_rate)\n",
    "        peak_amplitude = signals['EDA_Phasic'].max()\n",
    "        # this is not useful now, cause it's always one\n",
    "        # peak_frequency = len(peaks['SCR_Peaks']) / len(segment)\n",
    "\n",
    "        psd = nk.signal_psd(segment, sampling_rate=sampling_rate)\n",
    "\n",
    "        features = {\n",
    "            \"SCL\": scl,\n",
    "            \"SCR\": scr,\n",
    "            \"Peak Amplitude\": peak_amplitude,\n",
    "            \"PSD Total Power\": psd['Power'].sum(),\n",
    "            \"PSD LF Power\": psd['Power'][psd['Frequency'] <= 0.15].sum(),\n",
    "            \"PSD HF Power\": psd['Power'][(psd['Frequency'] > 0.15) & (psd['Frequency'] <= 0.4)].sum()\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported data type\")\n",
    "    return features\n",
    "\n",
    "def get_segment_timestamps(data):\n",
    "    \"\"\" Gets the df data based on the timestamps\n",
    "    (NOTE: this allows for further segmentation by being flexible and\n",
    "    not relying on adding new df columns or indices)\n",
    "    \"\"\"\n",
    "    # Get data only for videos 7 and 8 (scary videos)\n",
    "    scary_1_data = data[data['video'] == 7]\n",
    "    scary_2_data = data[data['video'] == 8]\n",
    "\n",
    "    # Get the start and end timestamps\n",
    "    scary_1_start = scary_1_data['daqtime'].iloc[0]\n",
    "    scary_1_end = scary_1_data['daqtime'].iloc[-1]\n",
    "    scary_2_start = scary_2_data['daqtime'].iloc[0]\n",
    "    scary_2_end = scary_2_data['daqtime'].iloc[-1]\n",
    "\n",
    "    # Swap the videos if needed (based on presented order)\n",
    "    if scary_1_start > scary_2_start: # compare start timestamps\n",
    "        (scary_1_start, scary_1_end), (scary_2_start, scary_2_end) = (scary_2_start, scary_2_end), (scary_1_start, scary_1_end)\n",
    "\n",
    "    return (scary_1_start, scary_1_end), (scary_2_start, scary_2_end)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------- Participant 1 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 2 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 3 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 4 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 5 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 6 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 7 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 8 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 9 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 10 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 11 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 12 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 13 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 14 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 15 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 16 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 17 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 18 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 19 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 20 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 21 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 22 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 23 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 24 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 25 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 26 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 27 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 28 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 29 data -----------------\n",
      "\n",
      "\n",
      "----------- Participant 30 data -----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for participant in range(len(physio_data)):\n",
    "    print(f\"\\n----------- Participant {participant+1} data -----------------\\n\")\n",
    "\n",
    "    ecg_signal = physio_data[participant]['ecg']\n",
    "    gsr_signal = physio_data[participant]['gsr']\n",
    "    original_timestamps = physio_data[participant]['daqtime']\n",
    "\n",
    "    # Clean signals (set to True to visualize before-after)\n",
    "    ecg_cleaned = preprocess_signal(ecg_signal, data_type=\"ecg\", visualization=False)\n",
    "    gsr_cleaned = preprocess_signal(gsr_signal, data_type=\"gsr\", visualization=False)\n",
    "\n",
    "    # Get timestamps for scary videos\n",
    "    (scary_1_start, scary_1_end), (scary_2_start, scary_2_end) = get_segment_timestamps(physio_data[participant])\n",
    "\n",
    "    # Extract segment data (NOTE: something like this could be used later even if we segment the videos further)\n",
    "    ecg_scary_1_segment = extract_segments(ecg_cleaned, scary_1_start, scary_1_end, original_timestamps)\n",
    "    ecg_scary_2_segment = extract_segments(ecg_cleaned, scary_2_start, scary_2_end, original_timestamps)\n",
    "\n",
    "    gsr_scary_1_segment = extract_segments(gsr_cleaned, scary_1_start, scary_1_end, original_timestamps)\n",
    "    gsr_scary_2_segment = extract_segments(gsr_cleaned, scary_2_start, scary_2_end, original_timestamps)\n",
    "\n",
    "    # Calculate ECG and GSR features for both videos\n",
    "    ecg_features_scary_1 = extract_features(ecg_scary_1_segment, data_type=\"ecg\")\n",
    "    ecg_features_scary_2 = extract_features(ecg_scary_2_segment, data_type=\"ecg\")\n",
    "\n",
    "    gsr_features_scary_1 = extract_features(gsr_scary_1_segment, data_type=\"gsr\")\n",
    "    gsr_features_scary_2 = extract_features(gsr_scary_2_segment, data_type=\"gsr\")\n",
    "\n",
    "    # Calculate differences for all features (to use in statistical analysis)\n",
    "    ecg_features_diff = {f'diff_ecg_{key}': ecg_features_scary_2[key] - ecg_features_scary_1[key] for key in ecg_features_scary_1.keys()}\n",
    "    gsr_features_diff = {f'diff_gsr_{key}': gsr_features_scary_2[key] - gsr_features_scary_1[key] for key in gsr_features_scary_1.keys()}\n",
    "\n",
    "    # Compile all features into a single dictionary\n",
    "    participant_features = {'participant': participant}\n",
    "    participant_features.update({f'scary_1_ecg_{key}': value for key, value in ecg_features_scary_1.items()})\n",
    "    participant_features.update({f'scary_2_ecg_{key}': value for key, value in ecg_features_scary_2.items()})\n",
    "    participant_features.update(ecg_features_diff)\n",
    "    participant_features.update({f'scary_1_gsr_{key}': value for key, value in gsr_features_scary_1.items()})\n",
    "    participant_features.update({f'scary_2_gsr_{key}': value for key, value in gsr_features_scary_2.items()})\n",
    "    participant_features.update(gsr_features_diff)\n",
    "\n",
    "    # Add to results\n",
    "    results.append(participant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ecg_gsr_features.csv.\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save\n",
    "output_filepath = 'ecg_gsr_features.csv'\n",
    "results_df.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_filepath}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
